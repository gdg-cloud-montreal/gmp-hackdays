{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to GMP Hack Days \u00b6","title":"Home"},{"location":"#welcome-to-gmp-hack-days","text":"","title":"Welcome to GMP Hack Days"},{"location":"gke-cost-allocation/","text":"GKE Cost Allocation \u00b6 1 Configure GKE Cost Allocation \u00b6 1.1 Enable Detailed usage cost data in BQ \u00b6 Ensure that you have completed the steps to Export detailed usage cost data to BigQuery . If your organization is already exporting data, you must have access to query the tables. 1.2 Create GKE Cluster on GCP with GKE Cost Allocation enabled \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud beta container clusters create k8s-cost-allocation-labs \\ --region us-central1 \\ --enable-ip-alias \\ --enable-network-policy \\ --machine-type \"n1-standard-4\" \\ --release-channel regular \\ --enable-cost-allocation gcloud container clusters get-credentials k8s-cost-allocation-labs --region us-central1 1.3 Deploy Test Application \u00b6 1.2.3 (Optional) Deploy onlineboutique application and observe app compute resources \u00b6 Deploy microservices application onlineboutique : Step 1 Create Namespace onlineboutique kubectl create ns onlineboutique Step 2 Deploy Microservice application git clone https://github.com/GoogleCloudPlatform/microservices-demo.git cd microservices-demo kubens onlineboutique kubectl apply -f ./release/kubernetes-manifests.yaml Step 3 Verify Deployment: kubectl get pods Step 4 Observe Detailed Billing Information for Namespaces: Step 5 Observe Detailed Billing Information for Pods: Cleanup \u00b6 Delete Kubernetes clusters as it will enquire cost: gcloud container clusters delete k8s-cost-allocation-labs --region us-central1","title":"GKE Cost Allocation"},{"location":"gke-cost-allocation/#gke-cost-allocation","text":"","title":"GKE Cost Allocation"},{"location":"gke-cost-allocation/#1-configure-gke-cost-allocation","text":"","title":"1 Configure GKE Cost Allocation"},{"location":"gke-cost-allocation/#11-enable-detailed-usage-cost-data-in-bq","text":"Ensure that you have completed the steps to Export detailed usage cost data to BigQuery . If your organization is already exporting data, you must have access to query the tables.","title":"1.1 Enable Detailed usage cost data in BQ"},{"location":"gke-cost-allocation/#12-create-gke-cluster-on-gcp-with-gke-cost-allocation-enabled","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud beta container clusters create k8s-cost-allocation-labs \\ --region us-central1 \\ --enable-ip-alias \\ --enable-network-policy \\ --machine-type \"n1-standard-4\" \\ --release-channel regular \\ --enable-cost-allocation gcloud container clusters get-credentials k8s-cost-allocation-labs --region us-central1","title":"1.2 Create GKE Cluster on GCP with GKE Cost Allocation enabled"},{"location":"gke-cost-allocation/#13-deploy-test-application","text":"","title":"1.3 Deploy Test Application"},{"location":"gke-cost-allocation/#123-optional-deploy-onlineboutique-application-and-observe-app-compute-resources","text":"Deploy microservices application onlineboutique : Step 1 Create Namespace onlineboutique kubectl create ns onlineboutique Step 2 Deploy Microservice application git clone https://github.com/GoogleCloudPlatform/microservices-demo.git cd microservices-demo kubens onlineboutique kubectl apply -f ./release/kubernetes-manifests.yaml Step 3 Verify Deployment: kubectl get pods Step 4 Observe Detailed Billing Information for Namespaces: Step 5 Observe Detailed Billing Information for Pods:","title":"1.2.3 (Optional) Deploy onlineboutique application and observe app compute resources"},{"location":"gke-cost-allocation/#cleanup","text":"Delete Kubernetes clusters as it will enquire cost: gcloud container clusters delete k8s-cost-allocation-labs --region us-central1","title":"Cleanup"},{"location":"gmp-alertmanager/","text":"","title":"Gmp alertmanager"},{"location":"gmp-for-gcp-cloud-resources/","text":"","title":"GMP quering Google Cloud Metrics with PromQL and Grafana (WIP)"},{"location":"gmp-multi-tenant/","text":"","title":"Centralized Multi-tenant GMP setup with GKE Clusters in Different Projects (WIP)"},{"location":"gmp-with-gkeautopilot/","text":"","title":"Gmp with gkeautopilot"},{"location":"gmp-with-gkestandard-custom/","text":"Scraping Metrics App metrics with GMP and Managed Collections \u00b6 1 Configure GMP and Managed Collections \u00b6 1.1 Create Regional GKE Cluster on GCP with GMP \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud beta container clusters create k8s-gmp-labs \\ --region us-central1 \\ --enable-ip-alias \\ --enable-network-policy \\ --num-nodes 1 \\ --machine-type \"e2-standard-4\" \\ --release-channel regular \\ --enable-managed-prometheus gcloud container clusters get-credentials k8s-gmp-labs --region us-central1 Step 3 List the Google Managed Prometheus (GMPs) Custom Resources Definitions (CRD's) kubectl get crd | grep monitoring clusterpodmonitorings.monitoring.googleapis.com 2022-07-12T20:26:52Z clusterrules.monitoring.googleapis.com 2022-07-12T20:26:52Z globalrules.monitoring.googleapis.com 2022-07-12T20:26:53Z operatorconfigs.monitoring.googleapis.com 2022-07-12T20:26:53Z podmonitorings.monitoring.googleapis.com 2022-07-12T20:26:53Z rules.monitoring.googleapis.com 2022-07-12T20:26:54Z 1.2 Deploy prom-example application and scrape custom metrics with PodMonitoring \u00b6 GMP out of the box doesn't scrape any metrics. In order to see any scraped Metrics in Prometheus UI or Grafana, we need to have deployed application and configure it as target for scraping using PodMonitoring CR. We going to start by deploying application example-app in default namespace and adding it as scraping target: Step 1 Deploy application example-app in default namespace: cat << EOF>> example-app.yaml apiVersion: apps/v1 kind: Deployment metadata: name: example-app spec: replicas: 3 selector: matchLabels: app: example-app template: metadata: labels: app: example-app spec: containers: - name: example-app image: fabxc/instrumented_app ports: - name: web containerPort: 8080 EOF cat << EOF>> example-svc.yaml kind: Service apiVersion: v1 metadata: name: example-app labels: app: example-app spec: selector: app: example-app ports: - name: web port: 8080 EOF kubens default kubectl apply -f example-app.yaml kubectl apply -f example-svc.yaml Step 2 Deploy PodMonitoring in default namespace cat << EOF>> example-pod-mon.yaml apiVersion: monitoring.googleapis.com/v1 kind: PodMonitoring metadata: name: example-app spec: selector: matchLabels: app: example-app endpoints: - port: web interval: 30s EOF kubectl apply -f example-pod-mon.yaml Verify monitoring has been created in default namespace kubectl get -A podmonitorings.monitoring.googleapis.com Note Creation of monitoring can be done in application namespace as well Step 6 Verify that your Prometheus data is being exported to Managed Service for Prometheus: In the Google Cloud console, go to Monitoring In the Monitoring navigation pane, click Managed Prometheus Run following queries: up http_requests_total{code=\"200\",method=\"get\"} Success We can see Prometheus data is being exported to Managed Service for Prometheus 2 Configure Prometheus UI and Grafana Dashboards \u00b6 2.1 Verify SA account has correct permissions. \u00b6 Option 1: Using GKE with default Compute Service Account, should have all required permissions as it is using Editor role (roles/editor) on the project. (without WI enabled) Option 2: Using GKE with custom Service Account for Nodes (without WI enabled) Make sure default compute SA has monitoring.metricWriter and monitoring.viewer roles: gcloud projects get-iam-policy $PROJECT_ID If you have Editor role or roles/monitoring.metricWriter and role: roles/monitoring.viewer skip the step. Otherwise add following roles tp SA. gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member='serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com' \\ --role=roles/monitoring.metricWriter gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member='serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com' \\ --role=roles/monitoring.viewer Option 3: If using GKE with WI, follow the Configure a service account for Workload Identity instructions Option 4: If using Non-GKE Kubernetes clusters, follow the Provide credentials explicitly instructions 2.2 Setup Prometheus UI for GMP \u00b6 In order to see if GMP is working and scraping metric, GCP provides Prometheus UI frontend image. To deploy the Prometheus UI for Managed Service for Prometheus, run the following commands. Step 1 Deploy the Prometheus UI frontend service inside of namespace where Grafana going to be installed (or already installed)) export GRAFANA_NAMESPACE=grafana curl https://raw.githubusercontent.com/GoogleCloudPlatform/prometheus-engine/v0.4.1/examples/frontend.yaml | sed 's/\\$PROJECT_ID/gcp-demos-331123/' | kubectl apply -n $GRAFANA_NAMESPACE -f - Step 2 Port-forward the frontend service to your local machine. The following example forwards the service to port 9090: kubectl -n $GRAFANA_NAMESPACE port-forward svc/frontend 8080:9090 You can access the Prometheus UI in your by using the Web Preview button. The following screenshot shows a table in the Prometheus UI that displays the up{cluster=\"k8s-gmp-labs\"} metric: Result We can see that all pods from example-app and prom-example are being scraped. Step 3 Observe and try to build a dashboards using on the exposed Prometheus metrics emitted by example-app app: The following metrics are exposed: version - of type gauge - containing the app version - as a constant metric value 1 and label version , representing this app version http_requests_total - of type counter - representing the total numbere of incoming HTTP requests The sample output of the /metric endpoint after 5 incoming HTTP requests shown below. Note: with no initial incoming request, only version metric is reported. # HELP http_requests_total Count of all HTTP requests # TYPE http_requests_total counter http_requests_total{code=\"200\",method=\"get\"} # HELP version Version information about this binary # TYPE version gauge version{version=\"v0.3.0\"} 1 2.3 Use Grafana with GMP \u00b6 2.3.1 Install Grafana \u00b6 Google Cloud APIs all require authentication using OAuth2; however, Grafana doesn't support OAuth2 authentication for Prometheus data sources. To use Grafana with Managed Service for Prometheus, we've deployed Prometheus UI as an authentication proxy in a previous step. Option 1: Configure existing Grafana Dashboard to use GMP If you would like to take advantage of you existing Dashboards and already deployed Grafana, the only steps needed is to create a Data Source for Google Managed Prometheus described in this reference Once configured you can redirect existing dashboards to the new Data Source , or create the new once. Option 2: Configure Grafana Dashboard that comes with GMP Configure Grafana Dashboard that comes with GMP, following this steps If you don't have a running Grafana deployment in your cluster, then you can create an ephemeral test deployment to experiment with. To create an ephemeral Grafana deployment, apply the Managed Service for Prometheus grafana.yaml manifest to your cluster, and port-forward the grafana service to your local machine. The following example forwards the service to port 3000. kubectl -n gmp-test apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/prometheus-engine/v0.4.1/examples/grafana.yaml kubectl -n $GRAFANA_NAMESPACE port-forward svc/grafana 8080:3000 Option 3: Use Grafana Helm Charts or Grafana Operator For production deployment it is preferred to use Grafana Helm Chart or Grafana Operator as you can define and recreate you dashboards via json or CRDs in consistent manner. 2.3.2 Configure Grafana to work with GMP \u00b6 To query Managed Service for Prometheus in Grafana by using the Prometheus UI as the authentication proxy, you must add new data source to Grafana. To add a data source for the managed service use following reference documenation 3 Verify GMP Cost \u00b6 Option 1 Estimate Cost based on ingested metrics. Step 1 Create a new Grafana Dashboard: Data Source: Managed Service for Prometheus Metric Browser: `rate(gcm_export_samples_exported_total{job=~\".+\",instance=~\".+\"}[5m])` Step: 10 Example calculation: N1. 770 Samples per second based on Dashboard value N2. 770 samples per second * 2628000 seconds/mo = 2023 B samples/mo N3. Since $0.15/million samples: first 0-50 billion samples# Reference N1 N4. We can calculate final cost: 2023 B samples/mo * 0.15 = ~303$ per month Option 2 View your Google Cloud bill In the Google Cloud console, go to the Billing page. If you have more than one billing account, select Go to linked billing account to view the current project's billing account. To locate a different billing account, select Manage billing accounts and choose the account for which you'd like to get usage reports. Select Reports. From the Services menu, select the Stackdriver Monitoring option. From the SKUs menu, select the following options: Prometheus Samples Ingested Monitoring API Requests 4 Cleanup \u00b6 Delete Kubernetes cluster as it will enquire cost both for GKE and GMP. Delete GKE cluster: gcloud container clusters delete k8s-gmp-labs --region us-central1","title":"GMP scraping Custom App metrics"},{"location":"gmp-with-gkestandard-custom/#scraping-metrics-app-metrics-with-gmp-and-managed-collections","text":"","title":"Scraping Metrics App metrics with GMP and Managed Collections"},{"location":"gmp-with-gkestandard-custom/#1-configure-gmp-and-managed-collections","text":"","title":"1 Configure GMP and Managed Collections"},{"location":"gmp-with-gkestandard-custom/#11-create-regional-gke-cluster-on-gcp-with-gmp","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud beta container clusters create k8s-gmp-labs \\ --region us-central1 \\ --enable-ip-alias \\ --enable-network-policy \\ --num-nodes 1 \\ --machine-type \"e2-standard-4\" \\ --release-channel regular \\ --enable-managed-prometheus gcloud container clusters get-credentials k8s-gmp-labs --region us-central1 Step 3 List the Google Managed Prometheus (GMPs) Custom Resources Definitions (CRD's) kubectl get crd | grep monitoring clusterpodmonitorings.monitoring.googleapis.com 2022-07-12T20:26:52Z clusterrules.monitoring.googleapis.com 2022-07-12T20:26:52Z globalrules.monitoring.googleapis.com 2022-07-12T20:26:53Z operatorconfigs.monitoring.googleapis.com 2022-07-12T20:26:53Z podmonitorings.monitoring.googleapis.com 2022-07-12T20:26:53Z rules.monitoring.googleapis.com 2022-07-12T20:26:54Z","title":"1.1 Create Regional GKE Cluster on GCP with GMP"},{"location":"gmp-with-gkestandard-custom/#12-deploy-prom-example-application-and-scrape-custom-metrics-with-podmonitoring","text":"GMP out of the box doesn't scrape any metrics. In order to see any scraped Metrics in Prometheus UI or Grafana, we need to have deployed application and configure it as target for scraping using PodMonitoring CR. We going to start by deploying application example-app in default namespace and adding it as scraping target: Step 1 Deploy application example-app in default namespace: cat << EOF>> example-app.yaml apiVersion: apps/v1 kind: Deployment metadata: name: example-app spec: replicas: 3 selector: matchLabels: app: example-app template: metadata: labels: app: example-app spec: containers: - name: example-app image: fabxc/instrumented_app ports: - name: web containerPort: 8080 EOF cat << EOF>> example-svc.yaml kind: Service apiVersion: v1 metadata: name: example-app labels: app: example-app spec: selector: app: example-app ports: - name: web port: 8080 EOF kubens default kubectl apply -f example-app.yaml kubectl apply -f example-svc.yaml Step 2 Deploy PodMonitoring in default namespace cat << EOF>> example-pod-mon.yaml apiVersion: monitoring.googleapis.com/v1 kind: PodMonitoring metadata: name: example-app spec: selector: matchLabels: app: example-app endpoints: - port: web interval: 30s EOF kubectl apply -f example-pod-mon.yaml Verify monitoring has been created in default namespace kubectl get -A podmonitorings.monitoring.googleapis.com Note Creation of monitoring can be done in application namespace as well Step 6 Verify that your Prometheus data is being exported to Managed Service for Prometheus: In the Google Cloud console, go to Monitoring In the Monitoring navigation pane, click Managed Prometheus Run following queries: up http_requests_total{code=\"200\",method=\"get\"} Success We can see Prometheus data is being exported to Managed Service for Prometheus","title":"1.2 Deploy prom-example application and scrape custom metrics with PodMonitoring"},{"location":"gmp-with-gkestandard-custom/#2-configure-prometheus-ui-and-grafana-dashboards","text":"","title":"2 Configure Prometheus UI and Grafana Dashboards"},{"location":"gmp-with-gkestandard-custom/#21-verify-sa-account-has-correct-permissions","text":"Option 1: Using GKE with default Compute Service Account, should have all required permissions as it is using Editor role (roles/editor) on the project. (without WI enabled) Option 2: Using GKE with custom Service Account for Nodes (without WI enabled) Make sure default compute SA has monitoring.metricWriter and monitoring.viewer roles: gcloud projects get-iam-policy $PROJECT_ID If you have Editor role or roles/monitoring.metricWriter and role: roles/monitoring.viewer skip the step. Otherwise add following roles tp SA. gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member='serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com' \\ --role=roles/monitoring.metricWriter gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member='serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com' \\ --role=roles/monitoring.viewer Option 3: If using GKE with WI, follow the Configure a service account for Workload Identity instructions Option 4: If using Non-GKE Kubernetes clusters, follow the Provide credentials explicitly instructions","title":"2.1 Verify SA account has correct permissions."},{"location":"gmp-with-gkestandard-custom/#22-setup-prometheus-ui-for-gmp","text":"In order to see if GMP is working and scraping metric, GCP provides Prometheus UI frontend image. To deploy the Prometheus UI for Managed Service for Prometheus, run the following commands. Step 1 Deploy the Prometheus UI frontend service inside of namespace where Grafana going to be installed (or already installed)) export GRAFANA_NAMESPACE=grafana curl https://raw.githubusercontent.com/GoogleCloudPlatform/prometheus-engine/v0.4.1/examples/frontend.yaml | sed 's/\\$PROJECT_ID/gcp-demos-331123/' | kubectl apply -n $GRAFANA_NAMESPACE -f - Step 2 Port-forward the frontend service to your local machine. The following example forwards the service to port 9090: kubectl -n $GRAFANA_NAMESPACE port-forward svc/frontend 8080:9090 You can access the Prometheus UI in your by using the Web Preview button. The following screenshot shows a table in the Prometheus UI that displays the up{cluster=\"k8s-gmp-labs\"} metric: Result We can see that all pods from example-app and prom-example are being scraped. Step 3 Observe and try to build a dashboards using on the exposed Prometheus metrics emitted by example-app app: The following metrics are exposed: version - of type gauge - containing the app version - as a constant metric value 1 and label version , representing this app version http_requests_total - of type counter - representing the total numbere of incoming HTTP requests The sample output of the /metric endpoint after 5 incoming HTTP requests shown below. Note: with no initial incoming request, only version metric is reported. # HELP http_requests_total Count of all HTTP requests # TYPE http_requests_total counter http_requests_total{code=\"200\",method=\"get\"} # HELP version Version information about this binary # TYPE version gauge version{version=\"v0.3.0\"} 1","title":"2.2 Setup Prometheus UI for GMP"},{"location":"gmp-with-gkestandard-custom/#23-use-grafana-with-gmp","text":"","title":"2.3 Use Grafana with GMP"},{"location":"gmp-with-gkestandard-custom/#231-install-grafana","text":"Google Cloud APIs all require authentication using OAuth2; however, Grafana doesn't support OAuth2 authentication for Prometheus data sources. To use Grafana with Managed Service for Prometheus, we've deployed Prometheus UI as an authentication proxy in a previous step. Option 1: Configure existing Grafana Dashboard to use GMP If you would like to take advantage of you existing Dashboards and already deployed Grafana, the only steps needed is to create a Data Source for Google Managed Prometheus described in this reference Once configured you can redirect existing dashboards to the new Data Source , or create the new once. Option 2: Configure Grafana Dashboard that comes with GMP Configure Grafana Dashboard that comes with GMP, following this steps If you don't have a running Grafana deployment in your cluster, then you can create an ephemeral test deployment to experiment with. To create an ephemeral Grafana deployment, apply the Managed Service for Prometheus grafana.yaml manifest to your cluster, and port-forward the grafana service to your local machine. The following example forwards the service to port 3000. kubectl -n gmp-test apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/prometheus-engine/v0.4.1/examples/grafana.yaml kubectl -n $GRAFANA_NAMESPACE port-forward svc/grafana 8080:3000 Option 3: Use Grafana Helm Charts or Grafana Operator For production deployment it is preferred to use Grafana Helm Chart or Grafana Operator as you can define and recreate you dashboards via json or CRDs in consistent manner.","title":"2.3.1 Install Grafana"},{"location":"gmp-with-gkestandard-custom/#232-configure-grafana-to-work-with-gmp","text":"To query Managed Service for Prometheus in Grafana by using the Prometheus UI as the authentication proxy, you must add new data source to Grafana. To add a data source for the managed service use following reference documenation","title":"2.3.2 Configure Grafana to work with GMP"},{"location":"gmp-with-gkestandard-custom/#3-verify-gmp-cost","text":"Option 1 Estimate Cost based on ingested metrics. Step 1 Create a new Grafana Dashboard: Data Source: Managed Service for Prometheus Metric Browser: `rate(gcm_export_samples_exported_total{job=~\".+\",instance=~\".+\"}[5m])` Step: 10 Example calculation: N1. 770 Samples per second based on Dashboard value N2. 770 samples per second * 2628000 seconds/mo = 2023 B samples/mo N3. Since $0.15/million samples: first 0-50 billion samples# Reference N1 N4. We can calculate final cost: 2023 B samples/mo * 0.15 = ~303$ per month Option 2 View your Google Cloud bill In the Google Cloud console, go to the Billing page. If you have more than one billing account, select Go to linked billing account to view the current project's billing account. To locate a different billing account, select Manage billing accounts and choose the account for which you'd like to get usage reports. Select Reports. From the Services menu, select the Stackdriver Monitoring option. From the SKUs menu, select the following options: Prometheus Samples Ingested Monitoring API Requests","title":"3 Verify GMP Cost"},{"location":"gmp-with-gkestandard-custom/#4-cleanup","text":"Delete Kubernetes cluster as it will enquire cost both for GKE and GMP. Delete GKE cluster: gcloud container clusters delete k8s-gmp-labs --region us-central1","title":"4 Cleanup"},{"location":"gmp-with-gkestandard-flientbit/","text":"","title":"GMP scraping Fluent-bit metrics (WIP)"},{"location":"gmp-with-gkestandard-fluentd/","text":"port: prometheus-metrics interval: 15s","title":"Gmp with gkestandard fluentd"},{"location":"gmp-with-gkestandard-nginxingress/","text":"","title":"GMP scraping Nginx-Ingress metrics (WIP)"},{"location":"gmp-with-kcc/","text":"Deploy GKE with GMP enabled via Kubernetes Config Connector \u00b6 To start we will need to create a cluster with the Config Connector Resources install, this can be GKE or a compliant K8s distributions (ie kind), see here for installation types. In this example I will be using the Manual installation type so we can use the latest version of Config Connector (1.91 at the time of this writing) and a GKE cluster so we can use Workload Identity. 1. Deploy Config Controller GKE Cluster \u00b6 Create a Kubernetes Cluster In this example I will highlight the two ways to install Config Connector. Option 1: Manual mode which can be used on both GKE clusters as well as other CNCF conformant clusters. Option 2: Using Config Controller which is a pre-configured managed cluster that comes pre-loaded with Config Connector as well as Config Sync and Policy Controller. Option 1: Manual Install \u00b6 Create a Kubernetes Cluster gcloud container clusters create kcc-configs --machine-type \"e2-standard-4\" --image-type \"COS_CONTAINERD\" \\ --num-nodes \"3\" --enable-ip-alias --project $PROJECT_ID --zone northamerica-northeast1-a \\ --workload-pool=${PROJECT_ID}.svc.id.goog gcloud container clusters get-credentials kcc-configs --region northamerica-northeast1-a Install Config Connector Step 1 Download the latest Config Connector operator gsutil cp gs://configconnector-operator/latest/release-bundle.tar.gz release-bundle.tar.gz Step 2 Extract Tar tar zxvf release-bundle.tar.gz Step 3 Install the Operator kubectl apply -f operator-system/configconnector-operator.yaml Step 4 Create an Identity gcloud iam service-accounts create gmp-demo Step 5 Assign IAM Permissions gcloud projects add-iam-policy-binding ${PROJECT_ID} \\ --member=\"serviceAccount:gmp-demo@${PROJECT_ID}.iam.gserviceaccount.com\" \\ --role=\"roles/editor\" Step 6 Bind SA to Kubernetes SA for Config Connector to use gcloud iam service-accounts add-iam-policy-binding \\ gmp-demo@${PROJECT_ID}.iam.gserviceaccount.com \\ --member=\"serviceAccount:${PROJECT_ID}.svc.id.goog[cnrm-system/cnrm-controller-manager]\" \\ --role=\"roles/iam.workloadIdentityUser\" Step 7 Create Config Connector manifest cat << EOF>> configconnector.yaml apiVersion: core.cnrm.cloud.google.com/v1beta1 kind: ConfigConnector metadata: # the name is restricted to ensure that there is only one # ConfigConnector resource installed in your cluster name: configconnector.core.cnrm.cloud.google.com spec: mode: cluster googleServiceAccount: \"gmp-demo@${PROJECT_ID}.iam.gserviceaccount.com\" EOF Step 8 Deploy Config Connector kubectl apply -f configconnector.yaml Step 9 Configure Namespace for the resources and annotate it. kubectl create namespace config-control kubectl annotate namespace \\ config-control cnrm.cloud.google.com/project-id=${PROJECT_ID} Option 2: Via Config Controller \u00b6 Create a Config Controller instance. For this I will assume you have a default network setup but you can use a network of your choosing using the --network=you-network --subnet=subnet . gcloud services enable krmapihosting.googleapis.com \\ container.googleapis.com \\ cloudresourcemanager.googleapis.com gcloud anthos config controller create gmp-recipes --location northamerica-northeast1 Authenticate with the Instance gcloud anthos config controller get-credentials gmp-recipes \\ --location northamerica-northeast1 Assign permissions to the Config Connector SA export SA_EMAIL=\"$(kubectl get ConfigConnectorContext -n config-control \\ -o jsonpath='{.items[0].spec.googleServiceAccount}' 2> /dev/null)\" gcloud projects add-iam-policy-binding \"${PROJECT_ID}\" \\ --member \"serviceAccount:${SA_EMAIL}\" \\ --role \"roles/owner\" \\ --project \"${PROJECT_ID}\" The install will take about 15 minutes as the cluster gets provisioned and the resources are installed. 2. Deploy the GMP Enabled GKE Cluster via Config Controller \u00b6 Create the Container Cluster manifest. cat >gmp-cluster.yaml <<EOL apiVersion: container.cnrm.cloud.google.com/v1beta1 kind: ContainerCluster metadata: labels: availability: dev target-audience: gdg-cloud-montreal name: gmp-enabled-cluster namespace: config-control spec: description: A GMP enabled cluster location: northamerica-northeast1-a initialNodeCount: 1 networkingMode: ROUTES clusterIpv4Cidr: 10.96.0.0/14 masterAuthorizedNetworksConfig: cidrBlocks: - displayName: Trusted external network cidrBlock: 10.2.0.0/16 addonsConfig: gcePersistentDiskCsiDriverConfig: enabled: true horizontalPodAutoscaling: disabled: true httpLoadBalancing: disabled: false loggingConfig: enableComponents: - \"SYSTEM_COMPONENTS\" - \"WORKLOADS\" monitoringConfig: enableComponents: - \"SYSTEM_COMPONENTS\" managedPrometheus: enabled: true workloadIdentityConfig: # Replace ${PROJECT_ID?} with your project ID. workloadPool: \"${PROJECT_ID}.svc.id.goog\" EOL Deploy the Manifest. kubectl apply -f gmp-cluster.yaml 3. Verifying Installation \u00b6 To verify the cluster is up and running we can run the following command or go to the gke console. kubectl get containerclusters -n config-control Output: NAME AGE READY STATUS STATUS AGE gmp-enabled-cluster 22m True UpToDate 3m3s Congrats, you've just deployed a GMP enabled cluster via Config Connector! 4. Cleanup for Option #1 \u00b6 Delete Kubernetes clusters as it will enquire cost both for GKE and GMP. gcloud container clusters delete kcc-configs --region northamerica-northeast1-a gcloud container clusters delete gmp-enabled-cluster --region northamerica-northeast1-a 5. Cleanup for Option #2 \u00b6","title":"Automate GMP deployument with KCC"},{"location":"gmp-with-kcc/#deploy-gke-with-gmp-enabled-via-kubernetes-config-connector","text":"To start we will need to create a cluster with the Config Connector Resources install, this can be GKE or a compliant K8s distributions (ie kind), see here for installation types. In this example I will be using the Manual installation type so we can use the latest version of Config Connector (1.91 at the time of this writing) and a GKE cluster so we can use Workload Identity.","title":"Deploy GKE with GMP enabled via Kubernetes Config Connector"},{"location":"gmp-with-kcc/#1-deploy-config-controller-gke-cluster","text":"Create a Kubernetes Cluster In this example I will highlight the two ways to install Config Connector. Option 1: Manual mode which can be used on both GKE clusters as well as other CNCF conformant clusters. Option 2: Using Config Controller which is a pre-configured managed cluster that comes pre-loaded with Config Connector as well as Config Sync and Policy Controller.","title":"1. Deploy Config Controller GKE Cluster"},{"location":"gmp-with-kcc/#option-1-manual-install","text":"Create a Kubernetes Cluster gcloud container clusters create kcc-configs --machine-type \"e2-standard-4\" --image-type \"COS_CONTAINERD\" \\ --num-nodes \"3\" --enable-ip-alias --project $PROJECT_ID --zone northamerica-northeast1-a \\ --workload-pool=${PROJECT_ID}.svc.id.goog gcloud container clusters get-credentials kcc-configs --region northamerica-northeast1-a Install Config Connector Step 1 Download the latest Config Connector operator gsutil cp gs://configconnector-operator/latest/release-bundle.tar.gz release-bundle.tar.gz Step 2 Extract Tar tar zxvf release-bundle.tar.gz Step 3 Install the Operator kubectl apply -f operator-system/configconnector-operator.yaml Step 4 Create an Identity gcloud iam service-accounts create gmp-demo Step 5 Assign IAM Permissions gcloud projects add-iam-policy-binding ${PROJECT_ID} \\ --member=\"serviceAccount:gmp-demo@${PROJECT_ID}.iam.gserviceaccount.com\" \\ --role=\"roles/editor\" Step 6 Bind SA to Kubernetes SA for Config Connector to use gcloud iam service-accounts add-iam-policy-binding \\ gmp-demo@${PROJECT_ID}.iam.gserviceaccount.com \\ --member=\"serviceAccount:${PROJECT_ID}.svc.id.goog[cnrm-system/cnrm-controller-manager]\" \\ --role=\"roles/iam.workloadIdentityUser\" Step 7 Create Config Connector manifest cat << EOF>> configconnector.yaml apiVersion: core.cnrm.cloud.google.com/v1beta1 kind: ConfigConnector metadata: # the name is restricted to ensure that there is only one # ConfigConnector resource installed in your cluster name: configconnector.core.cnrm.cloud.google.com spec: mode: cluster googleServiceAccount: \"gmp-demo@${PROJECT_ID}.iam.gserviceaccount.com\" EOF Step 8 Deploy Config Connector kubectl apply -f configconnector.yaml Step 9 Configure Namespace for the resources and annotate it. kubectl create namespace config-control kubectl annotate namespace \\ config-control cnrm.cloud.google.com/project-id=${PROJECT_ID}","title":"Option 1: Manual Install"},{"location":"gmp-with-kcc/#option-2-via-config-controller","text":"Create a Config Controller instance. For this I will assume you have a default network setup but you can use a network of your choosing using the --network=you-network --subnet=subnet . gcloud services enable krmapihosting.googleapis.com \\ container.googleapis.com \\ cloudresourcemanager.googleapis.com gcloud anthos config controller create gmp-recipes --location northamerica-northeast1 Authenticate with the Instance gcloud anthos config controller get-credentials gmp-recipes \\ --location northamerica-northeast1 Assign permissions to the Config Connector SA export SA_EMAIL=\"$(kubectl get ConfigConnectorContext -n config-control \\ -o jsonpath='{.items[0].spec.googleServiceAccount}' 2> /dev/null)\" gcloud projects add-iam-policy-binding \"${PROJECT_ID}\" \\ --member \"serviceAccount:${SA_EMAIL}\" \\ --role \"roles/owner\" \\ --project \"${PROJECT_ID}\" The install will take about 15 minutes as the cluster gets provisioned and the resources are installed.","title":"Option 2: Via Config Controller"},{"location":"gmp-with-kcc/#2-deploy-the-gmp-enabled-gke-cluster-via-config-controller","text":"Create the Container Cluster manifest. cat >gmp-cluster.yaml <<EOL apiVersion: container.cnrm.cloud.google.com/v1beta1 kind: ContainerCluster metadata: labels: availability: dev target-audience: gdg-cloud-montreal name: gmp-enabled-cluster namespace: config-control spec: description: A GMP enabled cluster location: northamerica-northeast1-a initialNodeCount: 1 networkingMode: ROUTES clusterIpv4Cidr: 10.96.0.0/14 masterAuthorizedNetworksConfig: cidrBlocks: - displayName: Trusted external network cidrBlock: 10.2.0.0/16 addonsConfig: gcePersistentDiskCsiDriverConfig: enabled: true horizontalPodAutoscaling: disabled: true httpLoadBalancing: disabled: false loggingConfig: enableComponents: - \"SYSTEM_COMPONENTS\" - \"WORKLOADS\" monitoringConfig: enableComponents: - \"SYSTEM_COMPONENTS\" managedPrometheus: enabled: true workloadIdentityConfig: # Replace ${PROJECT_ID?} with your project ID. workloadPool: \"${PROJECT_ID}.svc.id.goog\" EOL Deploy the Manifest. kubectl apply -f gmp-cluster.yaml","title":"2. Deploy the GMP Enabled GKE Cluster via Config Controller"},{"location":"gmp-with-kcc/#3-verifying-installation","text":"To verify the cluster is up and running we can run the following command or go to the gke console. kubectl get containerclusters -n config-control Output: NAME AGE READY STATUS STATUS AGE gmp-enabled-cluster 22m True UpToDate 3m3s Congrats, you've just deployed a GMP enabled cluster via Config Connector!","title":"3. Verifying Installation"},{"location":"gmp-with-kcc/#4-cleanup-for-option-1","text":"Delete Kubernetes clusters as it will enquire cost both for GKE and GMP. gcloud container clusters delete kcc-configs --region northamerica-northeast1-a gcloud container clusters delete gmp-enabled-cluster --region northamerica-northeast1-a","title":"4. Cleanup for Option #1"},{"location":"gmp-with-kcc/#5-cleanup-for-option-2","text":"","title":"5. Cleanup for Option #2"},{"location":"gmp-with-terraform/","text":"","title":"Gmp with terraform"},{"location":"prometheus-operator-to-gmp/","text":"Prometheus-operator scraping metrics to GMP \u00b6 1 Configure Prometheus Operator on GKE \u00b6 The Prometheus Operator is a tool developed and opnesourced by CoreOS that aims to automate manual operations of Prometheus and Alertmanager on Kubernetes using Kubernetes Custom Resource Definitions (CRDs). The Prometheus Operator provides easy monitoring definitions for Kubernetes services and deployment and management of Prometheus instances. Once installed, the Prometheus Operator provides the following features: Kubernetes Custom Resources: Use Kubernetes custom resources to deploy and manage Prometheus , Alertmanager , and related components. Simplified Deployment Configuration: Configure the fundamentals of Prometheus like versions, persistence, retention policies, and replicas from a native Kubernetes resource. Target Services via Labels: Automatically generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language. Objective: Explore the operators' deployment setup Configure ServiceMonitor , which declaratively specifies how groups of Kubernetes services should be monitored. The Operator automatically generates Prometheus scrape configuration based on the current state of the objects in the API server. Configure PrometheusRule and AlertmanagerConfig to be able to send Slack alerts 1.1 Create Regional GKE Cluster on GCP \u00b6 Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-prometheus-labs \\ --region us-central1 \\ --enable-ip-alias \\ --enable-network-policy \\ --num-nodes 1 \\ --machine-type \"e2-standard-4\" \\ --release-channel regular gcloud container clusters get-credentials k8s-prometheus-labs --region us-central1 Step 3: (Optional) Setup kubectx and kubens Info kubectx + kubens : Power tools for kubectl : - kubectx helps you switch between clusters back and forth - kubens helps you switch between Kubernetes namespaces smoothly Install As kubectl plugins(macOS & Linux) kubectl krew install ctx kubectl krew install ns Note Google Cloud Console comes with kubectx and kubens setup. However above steps can be used to setup your local shell. 1.2 Install Prometheus Operator and Grafana Helm Charts \u00b6 1.2.1 Install kube-prometheus-stack helm chart \u00b6 kube-prometheus kube-prometheus provides example configurations for a complete cluster monitoring stack based on Prometheus and the Prometheus Operator. This includes deployment of multiple Prometheus and Alertmanager instances, metrics exporters such as the node_exporter for gathering node metrics, scrape target configuration linking Prometheus to various metrics endpoints, and example alerting rules for notification of potential issues in the cluster. helm chart The prometheus-community/kube-prometheus-stack helm chart provides a similar feature set to kube-prometheus. For more information, please see the chart's readme Step 1 Configure Helm repository helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update Step 2 Fetch Helm repository to local filesystem mkdir ~/gmp-setup cd ~/gmp-setup helm pull prometheus-community/kube-prometheus-stack --version 35.6.0 tar -xvzf kube-prometheus-stack-35.6.0.tgz cd kube-prometheus-stack tree -L 2 Output: \u2500\u2500 charts \u2502 \u251c\u2500\u2500 grafana \u2502 \u251c\u2500\u2500 kube-state-metrics \u2502 \u2514\u2500\u2500 prometheus-node-exporter \u251c\u2500\u2500 crds \u2502 \u251c\u2500\u2500 crd-alertmanagerconfigs.yaml \u2502 \u251c\u2500\u2500 crd-alertmanagers.yaml \u2502 \u251c\u2500\u2500 crd-podmonitors.yaml \u2502 \u251c\u2500\u2500 crd-probes.yaml \u2502 \u251c\u2500\u2500 crd-prometheuses.yaml \u2502 \u251c\u2500\u2500 crd-prometheusrules.yaml \u2502 \u251c\u2500\u2500 crd-servicemonitors.yaml \u2502 \u2514\u2500\u2500 crd-thanosrulers.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 alertmanager \u2502 \u251c\u2500\u2500 exporters \u2502 \u2502 \u251c\u2500\u2500 core-dns \u2502 \u2502 \u251c\u2500\u2500 kube-api-server \u2502 \u2502 \u251c\u2500\u2500 kube-controller-manager \u2502 \u2502 \u251c\u2500\u2500 kube-dns \u2502 \u2502 \u251c\u2500\u2500 kube-etcd \u2502 \u2502 \u251c\u2500\u2500 kube-proxy \u2502 \u2502 \u251c\u2500\u2500 kube-scheduler \u2502 \u2502 \u251c\u2500\u2500 kube-state-metrics \u2502 \u2502 \u251c\u2500\u2500 kubelet \u2502 \u2502 \u2514\u2500\u2500 node-exporter \u2502 \u251c\u2500\u2500 grafana \u2502 \u251c\u2500\u2500 prometheus \u2502 \u2514\u2500\u2500 prometheus-operator \u2514\u2500\u2500 values.yaml Summary This chart is maintained by the Prometheus community and contains everything you need to get started including: prometheus operator alertmanager grafana with predefined dashboards prometheus-node-exporter - Prometheus exporter for hardware and OS metrics exposed by *NIX kernels, written in Go with pluggable metric collectors Kubernetes Control and Data plane exporters such as kube-api-server , core-dns , kube-controller-manager , kube-etcd , kube-scheduler , kubelet kube-state-metrics - is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects Step 3 Create custom Grafana configuration, that will allow to expose Grafana Dashboard: Option 1 Ingress cat << EOF>> grafana_values.yaml grafana: adminPassword: admin ingress: enabled: true path: /* pathType: ImplementationSpecific service: type: NodePort EOF Option 1 LoadBalancer cat << EOF>> grafana_values.yaml grafana: adminPassword: admin service: type: LoadBalancer EOF Step 4 Install Helm Chart to monitoring namespace kubectl create ns monitoring kubens monitoring helm install prometheus-stack prometheus-community/kube-prometheus-stack --version 35.6.0 --values grafana_values.yaml 1.2.2 Observe Grafana Dashboards \u00b6 Step 1: Locate Grafana Dashboard URL: kubectl get svc prometheus-stack-grafana kubectl get ing Step 2: Launch the Grafana Dashboard and see Predefined Dashboards: loadbalancer_ip We've setup admin user password as: admin . So we will use this values to login to Grafana dashboard: admin/admin Result You should see your Grafana interface Step 2 Access Grafana Dashboard and see Predefined Dashboards: Step 3 Observe following Dashboards: General /Kubernetes / Compute Resources / Cluster Summary Observe Usage, Totals, Quotas, Requests for Memory and CPU per namespace This values could be good input for Pods requests and limits measurements General /Kubernetes / Compute Resources / Nodes (Pods) Summary Observe CPU and Memory per Node General /Kubernetes / Compute Resources / Namespace (Pods) Summary Observe CPU and Memory per Pods Observe Dashboards for Control Plane monitoring: General / Kubernetes / API server General / Kubernetes / Kubelet General / Kubernetes / Scheduler General / Kubernetes / Controller Manager General / etcd Note some of the Dashboards (etcd, Scheduler, Controller Manager) are empty, this is because GKE is Managed Kubernetes so some metrics are not available for scraping. 1.2.3 (Optional) Deploy onlineboutique application and observe app compute resources \u00b6 Deploy microservices application onlineboutique : Step 1 Create Namespace onlineboutique kubectl create ns onlineboutique Step 2 Deploy Microservice application git clone https://github.com/GoogleCloudPlatform/microservices-demo.git cd microservices-demo kubens onlineboutique kubectl apply -f ./release/kubernetes-manifests.yaml Step 3 Verify Deployment: kubectl get pods Step 4 Observe following Dashboards: General /Kubernetes / Compute Resources / Namespace (Pods) General /Kubernetes / Compute Resources / Namespace (Workloads) Choose: * Relative time ranges: Last 15 minutes * Namespace: onlineboutique Summary Observe CPU and Memory per Pods 1.2.4 Deploy application and scrape custom metrics with PodMonitor \u00b6 Deploy application prom-example : Step 1 Create Namespace prom-test kubectl create ns prom-test kubens prom-test Step 2 Create deployment prom-example cat << EOF>> prom-example.yaml apiVersion: apps/v1 kind: Deployment metadata: name: prom-example labels: app: prom-example spec: selector: matchLabels: app: prom-example replicas: 3 template: metadata: labels: app: prom-example spec: containers: - image: nilebox/prometheus-example-app@sha256:dab60d038c5d6915af5bcbe5f0279a22b95a8c8be254153e22d7cd81b21b84c5 name: prom-example ports: - name: metrics containerPort: 1234 command: - \"/main\" - \"--process-metrics\" - \"--go-metrics\" EOF Step 4 Deploy prom-example application kubectl apply -f prom-example.yaml kubectl get pods Step 5 List the Prometheus Operator Custom Resources Definitions (CRD's) kubectl get crd | grep monitoring Step 6 Scrape Pod Metrics using PodMonitor crd The operator uses ServiceMonitors or PodMonitor to define a set of targets to be monitored by Prometheus. It uses label selectors to define which Services or Pod to monitor, the namespaces to look for, and the port on which the metrics are exposed. First let's find out how kind: Prometheus will select PodMonitor using serviceMonitorSelector ? kubens monitoring kubectl get prometheuses.monitoring.coreos.com prometheus-stack-kube-prom-prometheus -o yaml | grep -C5 podMonitorSelector Output: podMonitorSelector: matchLabels: release: prometheus-stack Result Prometheus Operator will select all podMonitor that match match label: release: prometheus-stack Create a file pod-monitor.yaml with the following content to add a PodMonitor so that the Prometheus server scrapes only its own metrics endpoints: Define a PodMonitor in a manifest file podmonitor.yaml by selecting app: prom-example labels in namespace prom-test and scrape metrics from port: metrics . Define PodMonitor labels as release: prometheus-stack , so that Prometheus operator can select them. cat << EOF>> podmonitor.yaml apiVersion: monitoring.coreos.com/v1 kind: PodMonitor metadata: name: prom-example labels: release: prometheus-stack spec: namespaceSelector: matchNames: - prom-test selector: matchLabels: app: prom-example podMetricsEndpoints: - port: metrics EOF kubectl apply -f podmonitor.yaml Step 3 Observe that PodMonitor been picked up by Prometheus Operator kubectl -n monitoring get secret prometheus-prometheus-stack-kube-prom-prometheus -ojson | jq -r '.data[\"prometheus.yaml.gz\"]' | base64 -d | gunzip | grep \"prom-example\" 1.2.5 Deploy application and scrape custom metrics with ServiceMonitor \u00b6 Step 1 Deploy application example-app in default namespace: cat << EOF>> example-app.yaml apiVersion: apps/v1 kind: Deployment metadata: name: example-app spec: replicas: 3 selector: matchLabels: app: example-app template: metadata: labels: app: example-app spec: containers: - name: example-app image: fabxc/instrumented_app ports: - name: web containerPort: 8080 EOF cat << EOF>> example-svc.yaml kind: Service apiVersion: v1 metadata: name: example-app labels: app: example-app spec: selector: app: example-app ports: - name: web port: 8080 EOF kubens default kubectl apply -f example-app.yaml kubectl apply -f example-svc.yaml Step 2 Deploy ServiceMonitor in monitoring namespace First let's find out how kind: Prometheus will select PodMonitor using serviceMonitorSelector ? kubens monitoring kubectl get prometheuses.monitoring.coreos.com prometheus-stack-kube-prom-prometheus -o yaml | grep -C5 serviceMonitorSelector Output: podMonitorSelector: matchLabels: release: prometheus-stack Result Prometheus Operator will select all podMonitor that match match label: release: prometheus-stack cat << EOF>> example-svc-mon.yaml apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: example-app labels: release: prometheus-stack spec: namespaceSelector: matchNames: - default selector: matchLabels: app: example-app endpoints: - port: web EOF kubens monitoring kubectl apply -f example-svc-mon.yaml Verify servicemonitor has been created in monitoring namespace kubectl get -A servicemonitor.monitoring.coreos.com Note Creation of servicemonitor can be done in application namespace as well Step 3 Observe that ServiceMonitor been picked up by Prometheus Operator kubectl -n monitoring get secret prometheus-prometheus-stack-kube-prom-prometheus -ojson | jq -r '.data[\"prometheus.yaml.gz\"]' | base64 -d | gunzip | grep \"example-app\" Step 4 Observe Prometheus UI and check that servicemonitor has been discovered under Targets and ServiceDiscovery : kubectl port-forward svc/prometheus-stack-kube-prom-prometheus 8080:9090 Step 5 Step 4 Observe Grafana UI and try to build a dashboards using on the exposed Prometheus metrics emitted by example-app app: The following metrics are exposed: version - of type gauge - containing the app version - as a constant metric value 1 and label version , representing this app version http_requests_total - of type counter - representing the total numbere of incoming HTTP requests The sample output of the /metric endpoint after 5 incoming HTTP requests shown below. Note: with no initial incoming request, only version metric is reported. # HELP http_requests_total Count of all HTTP requests # TYPE http_requests_total counter http_requests_total{code=\"200\",method=\"get\"} 5 # HELP version Version information about this binary # TYPE version gauge version{version=\"v0.3.0\"} 1 2 Migrate from Prometheus Operator to Google Managed Prometheus with self-deployed collection \u00b6 2.0 Verify SA account has correct permissions. \u00b6 Option 1: Using GKE with default Compute Service Account, should have all required permissions as it is using Editor role (roles/editor) on the project. (without WI enabled) Option 2: Using GKE with custom Service Account for Nodes (without WI enabled) Make sure default compute SA has monitoring.metricWriter and monitoring.viewer roles: gcloud projects get-iam-policy $PROJECT_ID If you have Editor role or roles/monitoring.metricWriter and role: roles/monitoring.viewer skip the step. Otherwise add following roles tp SA. gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member='serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com' \\ --role=roles/monitoring.metricWriter gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member='serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com' \\ --role=roles/monitoring.viewer Option 3: If using GKE with WI, follow the Configure a service account for Workload Identity instructions Option 4: If using Non-GKE Kubernetes clusters, follow the Provide credentials explicitly instructions 2.1 Setup GMP Collection \u00b6 While Prometheus Operator, provides us great APIs to declaratively configure Prometheus tasks such as scrape metrics and configure alerts and etc. Prometheus Operator itself doesn't solve problem of central monitoring system as it's deployed per K8s Cluster or per-namespace, and usually Organizations have many K8s Clusters and they looking for a central pain of glass view. Another challenge that Prometheus Operator doesn't solve is long term storage. To address above challenges we can use GMP with self-deployed collection. This way once can still use existing Prometheus Operator configuration using CRDs and Declarative APIs and take advantage of Google Managed Prometheus Datastore - based on - Monarch, that can handle incredible scale and store 2 years of metrics at no cost. See Reference doc - Get started with self-deployed collection Step 1 Create prometheus-stack Helm env var file, that will update from prometheus operator binary to GMP binary: cat << EOF>> update_to_gmp.yaml prometheus: prometheusSpec: image: repository: gke.gcr.io/prometheus-engine/prometheus tag: v2.35.0-gmp.2-gke.0 grafana: adminPassword: admin ingress: enabled: true path: /* pathType: ImplementationSpecific service: type: NodePort EOF Step 1 Update prometheus-stack Helm chart with GMP binary (image): kubectl get -A prometheuses.monitoring.coreos.com Output: image: \"quay.io/prometheus-operator/prometheus-operator:v0.56.3\" Step 2 Let's replace the prometheus-operator image with gmp image with Helm: helm upgrade prometheus-stack prometheus-community/kube-prometheus-stack --version 35.6.0 --values update_to_gmp.yaml Step 3 Verify that Prometheus Operator is configured with GMP binary: kubectl get -A prometheuses.monitoring.coreos.com Output: monitoring prometheus-stack-kube-prom-prometheus v2.35.0-gmp.2-gke.0 1 50m 2.2 Verify GMP Collection \u00b6 See Reference doc - Managed Service for Prometheus page Step 1 Verify that Prometheus data is being exported and stored at Managed Service for Prometheus Datastore: The simplest way to verify that your Prometheus data is being exported is to use the PromQL-based Managed Service for Prometheus page in the Google Cloud console. To view this page, do the following: In the Google Cloud console, go to Monitoring or use the following button: In the Monitoring navigation pane, click Managed Prometheus. On the Managed Service for Prometheus page, you can use PromQL queries to retrieve and chart data collected with the managed service. This page can query only data collected by Managed Service for Prometheus. The following screenshot shows a chart that displays the up metric To deploy the Prometheus UI for Managed Service for Prometheus, run the following commands: Deploy the frontend service and configure it to query the scoping project of your metrics scope of your choice: 2.3 Setup Prometheus UI for GMP \u00b6 To deploy the Prometheus UI for Managed Service for Prometheus, run the following commands. Step 1 Deploy the Prometheus UI frontend service inside of namespace where Grafana setup is running. Note If you want to continue using a Grafana deployment installed by kube-prometheus, then deploy the Prometheus UI in the monitoring namespace instead. However for production use cases Grafana typically deployed with Grafana Helm Chart or Grafana Operator Chart, allowing operators deploy their customer Dashboards. export GRAFANA_NAMESPACE=monitoring curl https://raw.githubusercontent.com/GoogleCloudPlatform/prometheus-engine/v0.4.1/examples/frontend.yaml | sed 's/\\$PROJECT_ID/gcp-demos-331123/' | kubectl apply -n $GRAFANA_NAMESPACE -f - Step 2 Port-forward the frontend service to your local machine. The following example forwards the service to port 9090: kubectl -n $GRAFANA_NAMESPACE port-forward svc/frontend 8080:9090 2.4 Setup Grafana for GMP \u00b6 Google Cloud APIs all require authentication using OAuth2; however, Grafana doesn't support OAuth2 authentication for Prometheus data sources. To use Grafana with Managed Service for Prometheus, we've deployed Prometheus UI as an authentication proxy in a previous step. Option 1: Configure existing Grafana Dashboard to use GMP If you would like to take advantage of you existing Dashboards and already deployed Grafana, the only steps needed is to create a Data Source for Google Managed Prometheus described in this reference Once configured you can redirect existing dashboards to the new Data Source , or create the new once. Option 2: Configure Grafana Dashboard that comes with GMP Configure Grafana Dashboard that comes with GMP, following this steps Note This option is good only for quick testing Grafana Option 3: Use Grafana Helm Charts or Grafana Operator For production deployment it is preferred to use Grafana Helm Chart or Grafana Operator as you can define and recreate you dashboards via json or CRDs in consistent manner. 2.5 Verify GMP Cost \u00b6 Option 1 Estimate Cost based on ingested metrics. Step 1 Create a new Grafana Dashboard: Data Source: Managed Service for Prometheus Metric Browser: `rate(gcm_export_samples_exported_total{job=~\".+\",instance=~\".+\"}[5m])` Step: 10 Example calculation: N1. 770 Samples per second based on Dashboard value N2. 770 samples per second * 2628000 seconds/mo = 2023 B samples/mo N3. Since $0.15/million samples: first 0-50 billion samples# Reference N1 N4. We can calculate final cost: 2023 B samples/mo * 0.15 = ~303$ per month Option 2 View your Google Cloud bill In the Google Cloud console, go to the Billing page. If you have more than one billing account, select Go to linked billing account to view the current project's billing account. To locate a different billing account, select Manage billing accounts and choose the account for which you'd like to get usage reports. Select Reports. From the Services menu, select the Stackdriver Monitoring option. From the SKUs menu, select the following options: Prometheus Samples Ingested Monitoring API Requests 7 Cleanup \u00b6 Delete Kubernetes cluster as it will enquire cost both for GKE and GMP. Uninstall Helm Charts: helm uninstall prometheus-stack -n monitoring kubectl delete crd alertmanagerconfigs.monitoring.coreos.com kubectl delete crd alertmanagers.monitoring.coreos.com kubectl delete crd podmonitors.monitoring.coreos.com kubectl delete crd probes.monitoring.coreos.com kubectl delete crd prometheuses.monitoring.coreos.com kubectl delete crd prometheusrules.monitoring.coreos.com kubectl delete crd servicemonitors.monitoring.coreos.com kubectl delete crd thanosrulers.monitoring.coreos.com Delete GKE cluster: gcloud container clusters delete k8s-prometheus-labs --region us-central1","title":"Prometheus-operator scraping metrics to GMP"},{"location":"prometheus-operator-to-gmp/#prometheus-operator-scraping-metrics-to-gmp","text":"","title":"Prometheus-operator scraping metrics to GMP"},{"location":"prometheus-operator-to-gmp/#1-configure-prometheus-operator-on-gke","text":"The Prometheus Operator is a tool developed and opnesourced by CoreOS that aims to automate manual operations of Prometheus and Alertmanager on Kubernetes using Kubernetes Custom Resource Definitions (CRDs). The Prometheus Operator provides easy monitoring definitions for Kubernetes services and deployment and management of Prometheus instances. Once installed, the Prometheus Operator provides the following features: Kubernetes Custom Resources: Use Kubernetes custom resources to deploy and manage Prometheus , Alertmanager , and related components. Simplified Deployment Configuration: Configure the fundamentals of Prometheus like versions, persistence, retention policies, and replicas from a native Kubernetes resource. Target Services via Labels: Automatically generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language. Objective: Explore the operators' deployment setup Configure ServiceMonitor , which declaratively specifies how groups of Kubernetes services should be monitored. The Operator automatically generates Prometheus scrape configuration based on the current state of the objects in the API server. Configure PrometheusRule and AlertmanagerConfig to be able to send Slack alerts","title":"1 Configure Prometheus Operator on GKE"},{"location":"prometheus-operator-to-gmp/#11-create-regional-gke-cluster-on-gcp","text":"Step 1 Enable the Google Kubernetes Engine API. gcloud services enable container.googleapis.com Step 2 From the cloud shell, run the following command to create a cluster with 1 node: gcloud container clusters create k8s-prometheus-labs \\ --region us-central1 \\ --enable-ip-alias \\ --enable-network-policy \\ --num-nodes 1 \\ --machine-type \"e2-standard-4\" \\ --release-channel regular gcloud container clusters get-credentials k8s-prometheus-labs --region us-central1 Step 3: (Optional) Setup kubectx and kubens Info kubectx + kubens : Power tools for kubectl : - kubectx helps you switch between clusters back and forth - kubens helps you switch between Kubernetes namespaces smoothly Install As kubectl plugins(macOS & Linux) kubectl krew install ctx kubectl krew install ns Note Google Cloud Console comes with kubectx and kubens setup. However above steps can be used to setup your local shell.","title":"1.1 Create Regional GKE Cluster on GCP"},{"location":"prometheus-operator-to-gmp/#12-install-prometheus-operator-and-grafana-helm-charts","text":"","title":"1.2 Install Prometheus Operator and Grafana Helm Charts"},{"location":"prometheus-operator-to-gmp/#121-install-kube-prometheus-stack-helm-chart","text":"kube-prometheus kube-prometheus provides example configurations for a complete cluster monitoring stack based on Prometheus and the Prometheus Operator. This includes deployment of multiple Prometheus and Alertmanager instances, metrics exporters such as the node_exporter for gathering node metrics, scrape target configuration linking Prometheus to various metrics endpoints, and example alerting rules for notification of potential issues in the cluster. helm chart The prometheus-community/kube-prometheus-stack helm chart provides a similar feature set to kube-prometheus. For more information, please see the chart's readme Step 1 Configure Helm repository helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update Step 2 Fetch Helm repository to local filesystem mkdir ~/gmp-setup cd ~/gmp-setup helm pull prometheus-community/kube-prometheus-stack --version 35.6.0 tar -xvzf kube-prometheus-stack-35.6.0.tgz cd kube-prometheus-stack tree -L 2 Output: \u2500\u2500 charts \u2502 \u251c\u2500\u2500 grafana \u2502 \u251c\u2500\u2500 kube-state-metrics \u2502 \u2514\u2500\u2500 prometheus-node-exporter \u251c\u2500\u2500 crds \u2502 \u251c\u2500\u2500 crd-alertmanagerconfigs.yaml \u2502 \u251c\u2500\u2500 crd-alertmanagers.yaml \u2502 \u251c\u2500\u2500 crd-podmonitors.yaml \u2502 \u251c\u2500\u2500 crd-probes.yaml \u2502 \u251c\u2500\u2500 crd-prometheuses.yaml \u2502 \u251c\u2500\u2500 crd-prometheusrules.yaml \u2502 \u251c\u2500\u2500 crd-servicemonitors.yaml \u2502 \u2514\u2500\u2500 crd-thanosrulers.yaml \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 NOTES.txt \u2502 \u251c\u2500\u2500 _helpers.tpl \u2502 \u251c\u2500\u2500 alertmanager \u2502 \u251c\u2500\u2500 exporters \u2502 \u2502 \u251c\u2500\u2500 core-dns \u2502 \u2502 \u251c\u2500\u2500 kube-api-server \u2502 \u2502 \u251c\u2500\u2500 kube-controller-manager \u2502 \u2502 \u251c\u2500\u2500 kube-dns \u2502 \u2502 \u251c\u2500\u2500 kube-etcd \u2502 \u2502 \u251c\u2500\u2500 kube-proxy \u2502 \u2502 \u251c\u2500\u2500 kube-scheduler \u2502 \u2502 \u251c\u2500\u2500 kube-state-metrics \u2502 \u2502 \u251c\u2500\u2500 kubelet \u2502 \u2502 \u2514\u2500\u2500 node-exporter \u2502 \u251c\u2500\u2500 grafana \u2502 \u251c\u2500\u2500 prometheus \u2502 \u2514\u2500\u2500 prometheus-operator \u2514\u2500\u2500 values.yaml Summary This chart is maintained by the Prometheus community and contains everything you need to get started including: prometheus operator alertmanager grafana with predefined dashboards prometheus-node-exporter - Prometheus exporter for hardware and OS metrics exposed by *NIX kernels, written in Go with pluggable metric collectors Kubernetes Control and Data plane exporters such as kube-api-server , core-dns , kube-controller-manager , kube-etcd , kube-scheduler , kubelet kube-state-metrics - is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects Step 3 Create custom Grafana configuration, that will allow to expose Grafana Dashboard: Option 1 Ingress cat << EOF>> grafana_values.yaml grafana: adminPassword: admin ingress: enabled: true path: /* pathType: ImplementationSpecific service: type: NodePort EOF Option 1 LoadBalancer cat << EOF>> grafana_values.yaml grafana: adminPassword: admin service: type: LoadBalancer EOF Step 4 Install Helm Chart to monitoring namespace kubectl create ns monitoring kubens monitoring helm install prometheus-stack prometheus-community/kube-prometheus-stack --version 35.6.0 --values grafana_values.yaml","title":"1.2.1 Install kube-prometheus-stack helm chart"},{"location":"prometheus-operator-to-gmp/#122-observe-grafana-dashboards","text":"Step 1: Locate Grafana Dashboard URL: kubectl get svc prometheus-stack-grafana kubectl get ing Step 2: Launch the Grafana Dashboard and see Predefined Dashboards: loadbalancer_ip We've setup admin user password as: admin . So we will use this values to login to Grafana dashboard: admin/admin Result You should see your Grafana interface Step 2 Access Grafana Dashboard and see Predefined Dashboards: Step 3 Observe following Dashboards: General /Kubernetes / Compute Resources / Cluster Summary Observe Usage, Totals, Quotas, Requests for Memory and CPU per namespace This values could be good input for Pods requests and limits measurements General /Kubernetes / Compute Resources / Nodes (Pods) Summary Observe CPU and Memory per Node General /Kubernetes / Compute Resources / Namespace (Pods) Summary Observe CPU and Memory per Pods Observe Dashboards for Control Plane monitoring: General / Kubernetes / API server General / Kubernetes / Kubelet General / Kubernetes / Scheduler General / Kubernetes / Controller Manager General / etcd Note some of the Dashboards (etcd, Scheduler, Controller Manager) are empty, this is because GKE is Managed Kubernetes so some metrics are not available for scraping.","title":"1.2.2 Observe Grafana Dashboards"},{"location":"prometheus-operator-to-gmp/#123-optional-deploy-onlineboutique-application-and-observe-app-compute-resources","text":"Deploy microservices application onlineboutique : Step 1 Create Namespace onlineboutique kubectl create ns onlineboutique Step 2 Deploy Microservice application git clone https://github.com/GoogleCloudPlatform/microservices-demo.git cd microservices-demo kubens onlineboutique kubectl apply -f ./release/kubernetes-manifests.yaml Step 3 Verify Deployment: kubectl get pods Step 4 Observe following Dashboards: General /Kubernetes / Compute Resources / Namespace (Pods) General /Kubernetes / Compute Resources / Namespace (Workloads) Choose: * Relative time ranges: Last 15 minutes * Namespace: onlineboutique Summary Observe CPU and Memory per Pods","title":"1.2.3 (Optional) Deploy onlineboutique application and observe app compute resources"},{"location":"prometheus-operator-to-gmp/#124-deploy-application-and-scrape-custom-metrics-with-podmonitor","text":"Deploy application prom-example : Step 1 Create Namespace prom-test kubectl create ns prom-test kubens prom-test Step 2 Create deployment prom-example cat << EOF>> prom-example.yaml apiVersion: apps/v1 kind: Deployment metadata: name: prom-example labels: app: prom-example spec: selector: matchLabels: app: prom-example replicas: 3 template: metadata: labels: app: prom-example spec: containers: - image: nilebox/prometheus-example-app@sha256:dab60d038c5d6915af5bcbe5f0279a22b95a8c8be254153e22d7cd81b21b84c5 name: prom-example ports: - name: metrics containerPort: 1234 command: - \"/main\" - \"--process-metrics\" - \"--go-metrics\" EOF Step 4 Deploy prom-example application kubectl apply -f prom-example.yaml kubectl get pods Step 5 List the Prometheus Operator Custom Resources Definitions (CRD's) kubectl get crd | grep monitoring Step 6 Scrape Pod Metrics using PodMonitor crd The operator uses ServiceMonitors or PodMonitor to define a set of targets to be monitored by Prometheus. It uses label selectors to define which Services or Pod to monitor, the namespaces to look for, and the port on which the metrics are exposed. First let's find out how kind: Prometheus will select PodMonitor using serviceMonitorSelector ? kubens monitoring kubectl get prometheuses.monitoring.coreos.com prometheus-stack-kube-prom-prometheus -o yaml | grep -C5 podMonitorSelector Output: podMonitorSelector: matchLabels: release: prometheus-stack Result Prometheus Operator will select all podMonitor that match match label: release: prometheus-stack Create a file pod-monitor.yaml with the following content to add a PodMonitor so that the Prometheus server scrapes only its own metrics endpoints: Define a PodMonitor in a manifest file podmonitor.yaml by selecting app: prom-example labels in namespace prom-test and scrape metrics from port: metrics . Define PodMonitor labels as release: prometheus-stack , so that Prometheus operator can select them. cat << EOF>> podmonitor.yaml apiVersion: monitoring.coreos.com/v1 kind: PodMonitor metadata: name: prom-example labels: release: prometheus-stack spec: namespaceSelector: matchNames: - prom-test selector: matchLabels: app: prom-example podMetricsEndpoints: - port: metrics EOF kubectl apply -f podmonitor.yaml Step 3 Observe that PodMonitor been picked up by Prometheus Operator kubectl -n monitoring get secret prometheus-prometheus-stack-kube-prom-prometheus -ojson | jq -r '.data[\"prometheus.yaml.gz\"]' | base64 -d | gunzip | grep \"prom-example\"","title":"1.2.4 Deploy application and scrape custom metrics with PodMonitor"},{"location":"prometheus-operator-to-gmp/#125-deploy-application-and-scrape-custom-metrics-with-servicemonitor","text":"Step 1 Deploy application example-app in default namespace: cat << EOF>> example-app.yaml apiVersion: apps/v1 kind: Deployment metadata: name: example-app spec: replicas: 3 selector: matchLabels: app: example-app template: metadata: labels: app: example-app spec: containers: - name: example-app image: fabxc/instrumented_app ports: - name: web containerPort: 8080 EOF cat << EOF>> example-svc.yaml kind: Service apiVersion: v1 metadata: name: example-app labels: app: example-app spec: selector: app: example-app ports: - name: web port: 8080 EOF kubens default kubectl apply -f example-app.yaml kubectl apply -f example-svc.yaml Step 2 Deploy ServiceMonitor in monitoring namespace First let's find out how kind: Prometheus will select PodMonitor using serviceMonitorSelector ? kubens monitoring kubectl get prometheuses.monitoring.coreos.com prometheus-stack-kube-prom-prometheus -o yaml | grep -C5 serviceMonitorSelector Output: podMonitorSelector: matchLabels: release: prometheus-stack Result Prometheus Operator will select all podMonitor that match match label: release: prometheus-stack cat << EOF>> example-svc-mon.yaml apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: example-app labels: release: prometheus-stack spec: namespaceSelector: matchNames: - default selector: matchLabels: app: example-app endpoints: - port: web EOF kubens monitoring kubectl apply -f example-svc-mon.yaml Verify servicemonitor has been created in monitoring namespace kubectl get -A servicemonitor.monitoring.coreos.com Note Creation of servicemonitor can be done in application namespace as well Step 3 Observe that ServiceMonitor been picked up by Prometheus Operator kubectl -n monitoring get secret prometheus-prometheus-stack-kube-prom-prometheus -ojson | jq -r '.data[\"prometheus.yaml.gz\"]' | base64 -d | gunzip | grep \"example-app\" Step 4 Observe Prometheus UI and check that servicemonitor has been discovered under Targets and ServiceDiscovery : kubectl port-forward svc/prometheus-stack-kube-prom-prometheus 8080:9090 Step 5 Step 4 Observe Grafana UI and try to build a dashboards using on the exposed Prometheus metrics emitted by example-app app: The following metrics are exposed: version - of type gauge - containing the app version - as a constant metric value 1 and label version , representing this app version http_requests_total - of type counter - representing the total numbere of incoming HTTP requests The sample output of the /metric endpoint after 5 incoming HTTP requests shown below. Note: with no initial incoming request, only version metric is reported. # HELP http_requests_total Count of all HTTP requests # TYPE http_requests_total counter http_requests_total{code=\"200\",method=\"get\"} 5 # HELP version Version information about this binary # TYPE version gauge version{version=\"v0.3.0\"} 1","title":"1.2.5 Deploy application and scrape custom metrics with ServiceMonitor"},{"location":"prometheus-operator-to-gmp/#2-migrate-from-prometheus-operator-to-google-managed-prometheus-with-self-deployed-collection","text":"","title":"2 Migrate from Prometheus Operator to Google Managed Prometheus with self-deployed collection"},{"location":"prometheus-operator-to-gmp/#20-verify-sa-account-has-correct-permissions","text":"Option 1: Using GKE with default Compute Service Account, should have all required permissions as it is using Editor role (roles/editor) on the project. (without WI enabled) Option 2: Using GKE with custom Service Account for Nodes (without WI enabled) Make sure default compute SA has monitoring.metricWriter and monitoring.viewer roles: gcloud projects get-iam-policy $PROJECT_ID If you have Editor role or roles/monitoring.metricWriter and role: roles/monitoring.viewer skip the step. Otherwise add following roles tp SA. gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member='serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com' \\ --role=roles/monitoring.metricWriter gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member='serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com' \\ --role=roles/monitoring.viewer Option 3: If using GKE with WI, follow the Configure a service account for Workload Identity instructions Option 4: If using Non-GKE Kubernetes clusters, follow the Provide credentials explicitly instructions","title":"2.0 Verify SA account has correct permissions."},{"location":"prometheus-operator-to-gmp/#21-setup-gmp-collection","text":"While Prometheus Operator, provides us great APIs to declaratively configure Prometheus tasks such as scrape metrics and configure alerts and etc. Prometheus Operator itself doesn't solve problem of central monitoring system as it's deployed per K8s Cluster or per-namespace, and usually Organizations have many K8s Clusters and they looking for a central pain of glass view. Another challenge that Prometheus Operator doesn't solve is long term storage. To address above challenges we can use GMP with self-deployed collection. This way once can still use existing Prometheus Operator configuration using CRDs and Declarative APIs and take advantage of Google Managed Prometheus Datastore - based on - Monarch, that can handle incredible scale and store 2 years of metrics at no cost. See Reference doc - Get started with self-deployed collection Step 1 Create prometheus-stack Helm env var file, that will update from prometheus operator binary to GMP binary: cat << EOF>> update_to_gmp.yaml prometheus: prometheusSpec: image: repository: gke.gcr.io/prometheus-engine/prometheus tag: v2.35.0-gmp.2-gke.0 grafana: adminPassword: admin ingress: enabled: true path: /* pathType: ImplementationSpecific service: type: NodePort EOF Step 1 Update prometheus-stack Helm chart with GMP binary (image): kubectl get -A prometheuses.monitoring.coreos.com Output: image: \"quay.io/prometheus-operator/prometheus-operator:v0.56.3\" Step 2 Let's replace the prometheus-operator image with gmp image with Helm: helm upgrade prometheus-stack prometheus-community/kube-prometheus-stack --version 35.6.0 --values update_to_gmp.yaml Step 3 Verify that Prometheus Operator is configured with GMP binary: kubectl get -A prometheuses.monitoring.coreos.com Output: monitoring prometheus-stack-kube-prom-prometheus v2.35.0-gmp.2-gke.0 1 50m","title":"2.1 Setup GMP Collection"},{"location":"prometheus-operator-to-gmp/#22-verify-gmp-collection","text":"See Reference doc - Managed Service for Prometheus page Step 1 Verify that Prometheus data is being exported and stored at Managed Service for Prometheus Datastore: The simplest way to verify that your Prometheus data is being exported is to use the PromQL-based Managed Service for Prometheus page in the Google Cloud console. To view this page, do the following: In the Google Cloud console, go to Monitoring or use the following button: In the Monitoring navigation pane, click Managed Prometheus. On the Managed Service for Prometheus page, you can use PromQL queries to retrieve and chart data collected with the managed service. This page can query only data collected by Managed Service for Prometheus. The following screenshot shows a chart that displays the up metric To deploy the Prometheus UI for Managed Service for Prometheus, run the following commands: Deploy the frontend service and configure it to query the scoping project of your metrics scope of your choice:","title":"2.2 Verify GMP Collection"},{"location":"prometheus-operator-to-gmp/#23-setup-prometheus-ui-for-gmp","text":"To deploy the Prometheus UI for Managed Service for Prometheus, run the following commands. Step 1 Deploy the Prometheus UI frontend service inside of namespace where Grafana setup is running. Note If you want to continue using a Grafana deployment installed by kube-prometheus, then deploy the Prometheus UI in the monitoring namespace instead. However for production use cases Grafana typically deployed with Grafana Helm Chart or Grafana Operator Chart, allowing operators deploy their customer Dashboards. export GRAFANA_NAMESPACE=monitoring curl https://raw.githubusercontent.com/GoogleCloudPlatform/prometheus-engine/v0.4.1/examples/frontend.yaml | sed 's/\\$PROJECT_ID/gcp-demos-331123/' | kubectl apply -n $GRAFANA_NAMESPACE -f - Step 2 Port-forward the frontend service to your local machine. The following example forwards the service to port 9090: kubectl -n $GRAFANA_NAMESPACE port-forward svc/frontend 8080:9090","title":"2.3 Setup Prometheus UI for GMP"},{"location":"prometheus-operator-to-gmp/#24-setup-grafana-for-gmp","text":"Google Cloud APIs all require authentication using OAuth2; however, Grafana doesn't support OAuth2 authentication for Prometheus data sources. To use Grafana with Managed Service for Prometheus, we've deployed Prometheus UI as an authentication proxy in a previous step. Option 1: Configure existing Grafana Dashboard to use GMP If you would like to take advantage of you existing Dashboards and already deployed Grafana, the only steps needed is to create a Data Source for Google Managed Prometheus described in this reference Once configured you can redirect existing dashboards to the new Data Source , or create the new once. Option 2: Configure Grafana Dashboard that comes with GMP Configure Grafana Dashboard that comes with GMP, following this steps Note This option is good only for quick testing Grafana Option 3: Use Grafana Helm Charts or Grafana Operator For production deployment it is preferred to use Grafana Helm Chart or Grafana Operator as you can define and recreate you dashboards via json or CRDs in consistent manner.","title":"2.4 Setup Grafana for GMP"},{"location":"prometheus-operator-to-gmp/#25-verify-gmp-cost","text":"Option 1 Estimate Cost based on ingested metrics. Step 1 Create a new Grafana Dashboard: Data Source: Managed Service for Prometheus Metric Browser: `rate(gcm_export_samples_exported_total{job=~\".+\",instance=~\".+\"}[5m])` Step: 10 Example calculation: N1. 770 Samples per second based on Dashboard value N2. 770 samples per second * 2628000 seconds/mo = 2023 B samples/mo N3. Since $0.15/million samples: first 0-50 billion samples# Reference N1 N4. We can calculate final cost: 2023 B samples/mo * 0.15 = ~303$ per month Option 2 View your Google Cloud bill In the Google Cloud console, go to the Billing page. If you have more than one billing account, select Go to linked billing account to view the current project's billing account. To locate a different billing account, select Manage billing accounts and choose the account for which you'd like to get usage reports. Select Reports. From the Services menu, select the Stackdriver Monitoring option. From the SKUs menu, select the following options: Prometheus Samples Ingested Monitoring API Requests","title":"2.5 Verify GMP Cost"},{"location":"prometheus-operator-to-gmp/#7-cleanup","text":"Delete Kubernetes cluster as it will enquire cost both for GKE and GMP. Uninstall Helm Charts: helm uninstall prometheus-stack -n monitoring kubectl delete crd alertmanagerconfigs.monitoring.coreos.com kubectl delete crd alertmanagers.monitoring.coreos.com kubectl delete crd podmonitors.monitoring.coreos.com kubectl delete crd probes.monitoring.coreos.com kubectl delete crd prometheuses.monitoring.coreos.com kubectl delete crd prometheusrules.monitoring.coreos.com kubectl delete crd servicemonitors.monitoring.coreos.com kubectl delete crd thanosrulers.monitoring.coreos.com Delete GKE cluster: gcloud container clusters delete k8s-prometheus-labs --region us-central1","title":"7 Cleanup"}]}